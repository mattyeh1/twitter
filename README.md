# ğŸ¦ Twitter Scraper - Sistema Avanzado

Sistema profesional de scraping de Twitter/X con **pool de drivers**, **scraping concurrente**, **jobs asÃ­ncronos** y **monitoreo en tiempo real**.

## ğŸš€ CaracterÃ­sticas

### **Mejoras Principales vs VersiÃ³n Anterior**

#### âœ… **Scraping Concurrente**
- **Antes**: Un solo usuario a la vez, los demÃ¡s esperaban
- **Ahora**: Hasta **3+ usuarios simultÃ¡neos** scrapeando en paralelo
- Pool de drivers Selenium con manejo automÃ¡tico de recursos

#### âœ… **Jobs AsÃ­ncronos con Celery**
- **Antes**: Request HTTP bloqueaba hasta terminar (2-5 minutos)
- **Ahora**: Response inmediata + progreso en tiempo real
- Sistema de colas con Redis
- Reintentos automÃ¡ticos en caso de error

#### âœ… **Monitoreo y MÃ©tricas**
- Dashboard de estadÃ­sticas del pool de drivers
- Monitor de tareas activas/en cola (Flower)
- Logs estructurados y detallados

#### âœ… **Arquitectura Profesional**
- **Antes**: Todo en 1 archivo de 2000+ lÃ­neas
- **Ahora**: Estructura modular y mantenible
- SeparaciÃ³n de responsabilidades (services, routes, models)
- ConfiguraciÃ³n centralizada

#### âœ… **FÃ¡cil Deployment**
- Docker Compose con todos los servicios
- Scripts de inicio automatizados
- ConfiguraciÃ³n con variables de entorno

---

## ğŸ“ Estructura del Proyecto

```
twitter-scraper/
â”œâ”€â”€ app/                          # AplicaciÃ³n Flask
â”‚   â”œâ”€â”€ __init__.py              # Factory de la app
â”‚   â”œâ”€â”€ routes/                  # Endpoints web
â”‚   â”‚   â”œâ”€â”€ dashboard.py         # Dashboard principal
â”‚   â”‚   â””â”€â”€ api.py               # API REST
â”‚   â”œâ”€â”€ services/                # LÃ³gica de negocio
â”‚   â”‚   â”œâ”€â”€ scraper_service.py   # Scraper refactorizado
â”‚   â”‚   â””â”€â”€ driver_pool.py       # Pool de Selenium drivers
â”‚   â”œâ”€â”€ templates/               # HTML templates
â”‚   â”‚   â”œâ”€â”€ base.html
â”‚   â”‚   â”œâ”€â”€ dashboard.html
â”‚   â”‚   â”œâ”€â”€ monitoring.html
â”‚   â”‚   â”œâ”€â”€ tweets.html
â”‚   â”‚   â””â”€â”€ search.html
â”‚   â””â”€â”€ static/                  # CSS, JS, imÃ¡genes
â”‚
â”œâ”€â”€ celery_app/                  # Celery para async tasks
â”‚   â”œâ”€â”€ celery_config.py         # ConfiguraciÃ³n de Celery
â”‚   â””â”€â”€ tasks.py                 # Tareas asÃ­ncronas
â”‚
â”œâ”€â”€ config/                      # ConfiguraciÃ³n
â”‚   â””â”€â”€ settings.py              # Settings centralizados
â”‚
â”œâ”€â”€ logs/                        # Logs de la aplicaciÃ³n
â”œâ”€â”€ chrome_profiles/             # Perfiles de Chrome (pool)
â”‚
â”œâ”€â”€ docker-compose.yml           # OrquestaciÃ³n de servicios
â”œâ”€â”€ Dockerfile                   # Imagen Docker
â”œâ”€â”€ requirements.txt             # Dependencias Python
â”œâ”€â”€ .env.example                 # Variables de entorno
â”œâ”€â”€ run.py                       # Punto de entrada
â”œâ”€â”€ start_local.sh               # Script de inicio (Linux/Mac)
â””â”€â”€ start_local.bat              # Script de inicio (Windows)
```

---

## ğŸ”§ InstalaciÃ³n

### **OpciÃ³n 1: Docker (Recomendado)**

```bash
# 1. Clonar repositorio
git clone <repo-url>
cd twitter-scraper

# 2. Copiar .env de ejemplo
cp .env.example .env

# 3. (Opcional) Editar .env con tus configuraciones
nano .env

# 4. Iniciar todos los servicios
docker-compose up -d

# 5. Ver logs
docker-compose logs -f

# Acceder a:
# - Dashboard: http://localhost:5000
# - Flower (Celery): http://localhost:5555
```

### **OpciÃ³n 2: InstalaciÃ³n Local**

#### **Requisitos previos:**
- Python 3.9+
- Redis
- Chrome/Chromium

#### **Pasos:**

```bash
# 1. Clonar repositorio
git clone <repo-url>
cd twitter-scraper

# 2. Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate    # Windows

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Instalar Redis
# Ubuntu/Debian:
sudo apt-get install redis-server
sudo systemctl start redis

# macOS:
brew install redis
brew services start redis

# Windows:
# Descargar de: https://github.com/microsoftarchive/redis/releases

# 5. Copiar configuraciÃ³n
cp .env.example .env

# 6. Iniciar servicios
# Linux/Mac:
chmod +x start_local.sh
./start_local.sh

# Windows:
start_local.bat
```

---

## ğŸ¯ Uso

### **1. Dashboard Principal**

Accede a `http://localhost:5000`

- Ver estadÃ­sticas generales
- Agregar nuevos perfiles a monitorear
- Scrapear perfiles manualmente
- Ver tweets guardados

### **2. Scraping Manual**

1. Click en "ğŸš€ Scrapear Ahora" en cualquier perfil
2. El sistema:
   - Encola la tarea en Celery
   - Adquiere un driver del pool
   - Scrapea en background
   - Muestra progreso en tiempo real
   - Actualiza dashboard automÃ¡ticamente

**MÃºltiples usuarios pueden scrapear simultÃ¡neamente** (hasta 3 por defecto)

### **3. BÃºsqueda de Tweets**

`http://localhost:5000/search`

- Buscar por palabras clave
- Filtrar por usuario, idioma, fecha
- Ordenar por fecha o likes
- Resaltado de tÃ©rminos buscados

### **4. Monitoreo del Sistema**

`http://localhost:5000/monitoring`

- Estado del pool de drivers (disponibles/activos)
- Tareas activas en Celery
- EstadÃ­sticas de uso
- MÃ©tricas en tiempo real

### **5. Flower (Celery Monitoring)**

`http://localhost:5555`

- Monitoreo avanzado de Celery
- Historial de tareas
- Workers activos
- GrÃ¡ficos de performance

---

## ğŸ”„ CÃ³mo Funciona el Pool de Drivers

### **Problema Original:**

```
Usuario A: Click "Scrapear @elonmusk"
  â”œâ”€ Abre Chrome
  â”œâ”€ Navega a x.com/elonmusk
  â”‚
  â”‚  Usuario B: Click "Scrapear @openai"
  â”‚   â”œâ”€ âŒ Intenta usar el mismo Chrome
  â”‚   â””â”€ âŒ CONFLICTO: Chrome ya estÃ¡ ocupado
  â”‚
  â””â”€ Usuario A termina (2-5 min despuÃ©s)
```

### **SoluciÃ³n con Pool:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      DRIVER POOL (Size: 3)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Driver 1 â”‚ Driver 2 â”‚ Driver 3  â”‚
â”‚ (libre)  â”‚ (libre)  â”‚ (libre)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Usuario A scrapea @elonmusk
  â†’ Adquiere Driver 1

Usuario B scrapea @openai (simultÃ¡neamente)
  â†’ Adquiere Driver 2

Usuario C scrapea @python (simultÃ¡neamente)
  â†’ Adquiere Driver 3

Usuario D scrapea @netflix
  â†’ Espera en cola (todos ocupados)
  â†’ Cuando A termina, obtiene Driver 1
```

---

## âš™ï¸ ConfiguraciÃ³n

### **Variables de Entorno (`.env`)**

```bash
# Pool de Drivers
DRIVER_POOL_SIZE=3              # Scrapers concurrentes
HEADLESS=True                   # Chrome sin GUI

# Celery
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Scraping
MAX_TWEETS_PER_SCRAPE=100
SCRAPE_SCROLL_COUNT=15
SCRAPE_SCROLL_DELAY=6

# Flask
FLASK_PORT=5000
FLASK_DEBUG=False
```

### **Ajustar Concurrencia**

**Para mÃ¡s scrapers simultÃ¡neos:**

```bash
# .env
DRIVER_POOL_SIZE=5  # Hasta 5 simultÃ¡neos

# docker-compose.yml (celery_worker)
command: celery -A celery_app.celery_config worker --concurrency=5
```

âš ï¸ **Nota**: MÃ¡s drivers = mÃ¡s RAM/CPU

---

## ğŸ“Š Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   USUARIOS   â”‚
â”‚ (navegador)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    FLASK     â”‚  â† Web UI + API REST
â”‚   (Puerto    â”‚
â”‚    5000)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º POST /scrape_now/username
       â”‚     â””â”€â–º Encola tarea en Celery
       â”‚
       â”œâ”€â”€â–º GET /api/task/<id>
       â”‚     â””â”€â–º Consulta progreso
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    REDIS     â”‚  â† Message Broker + Result Backend
â”‚   (Puerto    â”‚
â”‚    6379)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CELERY WORKERS               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚Worker 1â”‚Worker 2â”‚Worker 3â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”˜       â”‚
â”‚       â”‚        â”‚        â”‚            â”‚
â”‚       â–¼        â–¼        â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     DRIVER POOL            â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â” â”‚     â”‚
â”‚  â”‚  â”‚Chrome 1â”‚Chrome 2â”‚Chr 3â”‚ â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜ â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TWITTER/X  â”‚  â† Scraping target
â”‚   (x.com)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› Troubleshooting

### **Redis no se conecta**

```bash
# Verificar que Redis estÃ¡ corriendo
redis-cli ping
# Debe responder: PONG

# Iniciar Redis
sudo systemctl start redis  # Linux
brew services start redis   # macOS
```

### **Chrome no se encuentra**

```bash
# Instalar Chrome/Chromium
sudo apt-get install google-chrome-stable  # Ubuntu
brew install --cask google-chrome          # macOS

# O usar Chromium
sudo apt-get install chromium-browser
```

### **Error: "No driver available"**

- Todos los drivers estÃ¡n ocupados
- Espera a que se libere uno, o
- Aumenta `DRIVER_POOL_SIZE` en `.env`

### **Tareas quedan en "PENDING"**

```bash
# Verificar que Celery worker estÃ¡ corriendo
celery -A celery_app.celery_config inspect active

# Reiniciar worker
docker-compose restart celery_worker
```

---

## ğŸ“ˆ Escalabilidad

### **Escalar Workers**

```yaml
# docker-compose.yml
celery_worker_1:
  # ... config ...
  command: celery -A celery_app.celery_config worker --concurrency=3

celery_worker_2:
  # ... config ...
  command: celery -A celery_app.celery_config worker --concurrency=3

celery_worker_3:
  # ... config ...
  command: celery -A celery_app.celery_config worker --concurrency=3
```

Ahora puedes scrapear **9 perfiles simultÃ¡neamente** (3 workers Ã— 3 concurrency)

### **Escalar en la Nube**

- Deploy workers en mÃºltiples servidores
- Usar Redis en la nube (AWS ElastiCache, Redis Cloud)
- PostgreSQL en vez de SQLite

---

## ğŸ”’ Seguridad

**TODO (para producciÃ³n):**

- [ ] Agregar autenticaciÃ³n (Flask-Login)
- [ ] HTTPS con certificados SSL
- [ ] Rate limiting (Flask-Limiter)
- [ ] ValidaciÃ³n de inputs (Pydantic)
- [ ] CSRF protection
- [ ] Secrets en vault (no en .env)

---

## ğŸ“ Logs

Los logs se guardan en:

```
logs/
â”œâ”€â”€ app.log          # Flask app logs
â””â”€â”€ celery.log       # Celery worker logs
```

Ver logs en tiempo real:

```bash
# Docker
docker-compose logs -f web
docker-compose logs -f celery_worker

# Local
tail -f logs/app.log
```

---

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agrega nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

---

## ğŸ“„ Licencia

MIT License - Ver `LICENSE` para mÃ¡s detalles

---

## ğŸ‰ CrÃ©ditos

Desarrollado con â¤ï¸ por [Tu Nombre]

**Stack TecnolÃ³gico:**
- Flask (Web Framework)
- Celery (Async Tasks)
- Redis (Message Broker)
- Selenium (Web Scraping)
- BeautifulSoup (HTML Parsing)
- Docker (Containerization)

---

## ğŸ“ Soporte

Â¿Problemas? Abre un [Issue](https://github.com/tu-usuario/twitter-scraper/issues)

---

**Â¡Feliz Scraping! ğŸš€**
