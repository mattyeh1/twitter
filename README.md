# ğŸ¦ Twitter Scraper - Sistema Avanzado

Sistema profesional de scraping de Twitter/X con **pool de drivers**, **scraping concurrente**, **jobs asÃ­ncronos** y **monitoreo en tiempo real**.

## ğŸš€ CaracterÃ­sticas

### **Mejoras Principales vs VersiÃ³n Anterior**

#### âœ… **Scraping Concurrente**
- **Antes**: Un solo usuario a la vez, los demÃ¡s esperaban
- **Ahora**: Hasta **3+ usuarios simultÃ¡neos** scrapeando en paralelo
- Pool de drivers Selenium con manejo automÃ¡tico de recursos

#### âœ… **Jobs AsÃ­ncronos con Celery**
- **Antes**: Request HTTP bloqueaba hasta terminar (2-5 minutos)
- **Ahora**: Response inmediata + progreso en tiempo real
- Sistema de colas con Redis
- Reintentos automÃ¡ticos en caso de error

#### âœ… **Monitoreo y MÃ©tricas**
- Dashboard de estadÃ­sticas del pool de drivers
- Monitor de tareas activas/en cola (Flower)
- Logs estructurados y detallados

#### âœ… **Arquitectura Profesional**
- **Antes**: Todo en 1 archivo de 2000+ lÃ­neas
- **Ahora**: Estructura modular y mantenible
- SeparaciÃ³n de responsabilidades (services, routes, models)
- ConfiguraciÃ³n centralizada

#### âœ… **FÃ¡cil Deployment**
- Docker Compose con todos los servicios
- Scripts de inicio automatizados
- ConfiguraciÃ³n con variables de entorno

---

## ğŸ“ Estructura del Proyecto

```
twitter-scraper/
â”œâ”€â”€ app/                          # AplicaciÃ³n Flask
â”‚   â”œâ”€â”€ __init__.py              # Factory de la app
â”‚   â”œâ”€â”€ routes/                  # Endpoints web
â”‚   â”‚   â”œâ”€â”€ dashboard.py         # Dashboard principal
â”‚   â”‚   â””â”€â”€ api.py               # API REST
â”‚   â”œâ”€â”€ services/                # LÃ³gica de negocio
â”‚   â”‚   â”œâ”€â”€ scraper_service.py   # Scraper refactorizado
â”‚   â”‚   â””â”€â”€ driver_pool.py       # Pool de Selenium drivers
â”‚   â”œâ”€â”€ templates/               # HTML templates
â”‚   â”‚   â”œâ”€â”€ base.html
â”‚   â”‚   â”œâ”€â”€ dashboard.html
â”‚   â”‚   â”œâ”€â”€ monitoring.html
â”‚   â”‚   â”œâ”€â”€ tweets.html
â”‚   â”‚   â””â”€â”€ search.html
â”‚   â””â”€â”€ static/                  # CSS, JS, imÃ¡genes
â”‚
â”œâ”€â”€ celery_app/                  # Celery para async tasks
â”‚   â”œâ”€â”€ celery_config.py         # ConfiguraciÃ³n de Celery
â”‚   â””â”€â”€ tasks.py                 # Tareas asÃ­ncronas
â”‚
â”œâ”€â”€ config/                      # ConfiguraciÃ³n
â”‚   â””â”€â”€ settings.py              # Settings centralizados
â”‚
â”œâ”€â”€ logs/                        # Logs de la aplicaciÃ³n
â”œâ”€â”€ chrome_profiles/             # Perfiles de Chrome (pool)
â”‚
â”œâ”€â”€ docker-compose.yml           # OrquestaciÃ³n de servicios
â”œâ”€â”€ Dockerfile                   # Imagen Docker
â”œâ”€â”€ requirements.txt             # Dependencias Python
â”œâ”€â”€ .env.example                 # Variables de entorno
â”œâ”€â”€ run.py                       # Punto de entrada
â”œâ”€â”€ start_local.sh               # Script de inicio (Linux/Mac)
â””â”€â”€ start_local.bat              # Script de inicio (Windows)
```

---

## ğŸ”§ InstalaciÃ³n

### **OpciÃ³n 1: Docker (Recomendado)**

```bash
# 1. Clonar repositorio
git clone <repo-url>
cd twitter-scraper

# 2. Copiar .env de ejemplo
cp .env.example .env

# 3. (Opcional) Editar .env con tus configuraciones
nano .env

# 4. Iniciar todos los servicios
docker-compose up -d

# 5. Ver logs
docker-compose logs -f

# Acceder a:
# - Dashboard: http://localhost:5000
# - Flower (Celery): http://localhost:5555
```

### **OpciÃ³n 2: InstalaciÃ³n Local (Windows)**

#### **Requisitos previos:**
- Python 3.8+ (recomendado 3.10 o 3.11, evitar 3.14 pre-release)
- Redis para Windows
- Chrome + ChromeDriver

#### **Pasos:**

```bash
# 1. Instalar Redis para Windows
# Descargar e instalar de: https://github.com/tporadowski/redis/releases
# DespuÃ©s de instalar, iniciar Redis:
redis-server

# 2. Clonar repositorio
git clone <repo-url>
cd twitter

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Verificar instalaciÃ³n
python VERIFICAR_SISTEMA.py
# O usar el batch file:
VERIFICAR_SISTEMA.bat

# 5. Si todo OK, iniciar servicios
# OPCIÃ“N A: Inicio completo (instala + inicia)
start_windows.bat

# OPCIÃ“N B: Inicio rÃ¡pido (solo inicia)
INICIO_RAPIDO.bat
```

### **OpciÃ³n 3: InstalaciÃ³n Local (Linux/Mac)**

```bash
# 1. Instalar Redis
# Ubuntu/Debian:
sudo apt-get install redis-server
sudo systemctl start redis

# macOS:
brew install redis
brew services start redis

# 2. Clonar repositorio
git clone <repo-url>
cd twitter

# 3. Crear entorno virtual
python -m venv venv
source venv/bin/activate

# 4. Instalar dependencias
pip install -r requirements.txt

# 5. Copiar configuraciÃ³n
cp .env.example .env

# 6. Verificar instalaciÃ³n
python VERIFICAR_SISTEMA.py

# 7. Iniciar servicios manualmente
# Terminal 1: Flask
python run.py

# Terminal 2: Celery Worker
celery -A celery_app.celery_config worker --loglevel=info

# Terminal 3 (opcional): Flower
celery -A celery_app.celery_config flower --port=5555
```

---

## ğŸ¯ Uso

### **1. Dashboard Principal**

Accede a `http://localhost:5000`

- Ver estadÃ­sticas generales
- Agregar nuevos perfiles a monitorear
- Scrapear perfiles manualmente
- Ver tweets guardados

### **2. Scraping Manual**

1. Click en "ğŸš€ Scrapear Ahora" en cualquier perfil
2. El sistema:
   - Encola la tarea en Celery
   - Adquiere un driver del pool
   - Scrapea en background
   - Muestra progreso en tiempo real
   - Actualiza dashboard automÃ¡ticamente

**MÃºltiples usuarios pueden scrapear simultÃ¡neamente** (hasta 3 por defecto)

### **3. BÃºsqueda de Tweets**

`http://localhost:5000/search`

- Buscar por palabras clave
- Filtrar por usuario, idioma, fecha
- Ordenar por fecha o likes
- Resaltado de tÃ©rminos buscados

### **4. Monitoreo del Sistema**

`http://localhost:5000/monitoring`

- Estado del pool de drivers (disponibles/activos)
- Tareas activas en Celery
- EstadÃ­sticas de uso
- MÃ©tricas en tiempo real

### **5. Flower (Celery Monitoring)**

`http://localhost:5555`

- Monitoreo avanzado de Celery
- Historial de tareas
- Workers activos
- GrÃ¡ficos de performance

---

## ğŸ”„ CÃ³mo Funciona el Pool de Drivers

### **Problema Original:**

```
Usuario A: Click "Scrapear @elonmusk"
  â”œâ”€ Abre Chrome
  â”œâ”€ Navega a x.com/elonmusk
  â”‚
  â”‚  Usuario B: Click "Scrapear @openai"
  â”‚   â”œâ”€ âŒ Intenta usar el mismo Chrome
  â”‚   â””â”€ âŒ CONFLICTO: Chrome ya estÃ¡ ocupado
  â”‚
  â””â”€ Usuario A termina (2-5 min despuÃ©s)
```

### **SoluciÃ³n con Pool:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      DRIVER POOL (Size: 3)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Driver 1 â”‚ Driver 2 â”‚ Driver 3  â”‚
â”‚ (libre)  â”‚ (libre)  â”‚ (libre)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Usuario A scrapea @elonmusk
  â†’ Adquiere Driver 1

Usuario B scrapea @openai (simultÃ¡neamente)
  â†’ Adquiere Driver 2

Usuario C scrapea @python (simultÃ¡neamente)
  â†’ Adquiere Driver 3

Usuario D scrapea @netflix
  â†’ Espera en cola (todos ocupados)
  â†’ Cuando A termina, obtiene Driver 1
```

---

## âš™ï¸ ConfiguraciÃ³n

### **Variables de Entorno (`.env`)**

```bash
# Pool de Drivers
DRIVER_POOL_SIZE=3              # Scrapers concurrentes
HEADLESS=True                   # Chrome sin GUI

# Celery
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Scraping
MAX_TWEETS_PER_SCRAPE=100
SCRAPE_SCROLL_COUNT=15
SCRAPE_SCROLL_DELAY=6

# Flask
FLASK_PORT=5000
FLASK_DEBUG=False
```

### **Ajustar Concurrencia**

**Para mÃ¡s scrapers simultÃ¡neos:**

```bash
# .env
DRIVER_POOL_SIZE=5  # Hasta 5 simultÃ¡neos

# docker-compose.yml (celery_worker)
command: celery -A celery_app.celery_config worker --concurrency=5
```

âš ï¸ **Nota**: MÃ¡s drivers = mÃ¡s RAM/CPU

---

## ğŸ“Š Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   USUARIOS   â”‚
â”‚ (navegador)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    FLASK     â”‚  â† Web UI + API REST
â”‚   (Puerto    â”‚
â”‚    5000)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º POST /scrape_now/username
       â”‚     â””â”€â–º Encola tarea en Celery
       â”‚
       â”œâ”€â”€â–º GET /api/task/<id>
       â”‚     â””â”€â–º Consulta progreso
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    REDIS     â”‚  â† Message Broker + Result Backend
â”‚   (Puerto    â”‚
â”‚    6379)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CELERY WORKERS               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚Worker 1â”‚Worker 2â”‚Worker 3â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”˜       â”‚
â”‚       â”‚        â”‚        â”‚            â”‚
â”‚       â–¼        â–¼        â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚     DRIVER POOL            â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â” â”‚     â”‚
â”‚  â”‚  â”‚Chrome 1â”‚Chrome 2â”‚Chr 3â”‚ â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜ â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TWITTER/X  â”‚  â† Scraping target
â”‚   (x.com)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› Troubleshooting

### **ğŸ” VerificaciÃ³n del Sistema**

Antes de reportar errores, ejecuta el script de verificaciÃ³n:

```bash
python VERIFICAR_SISTEMA.py
# O:
VERIFICAR_SISTEMA.bat
```

Este script verifica:
- âœ“ Python 3.8+
- âœ“ Redis corriendo
- âœ“ Dependencias instaladas
- âœ“ Archivo .env configurado
- âœ“ Directorios creados
- âœ“ ChromeDriver funcional
- âœ“ Celery configurado
- âœ“ Flask app cargable
- âœ“ Redis Python conexiÃ³n

### **Redis no se conecta**

```bash
# Verificar que Redis estÃ¡ corriendo
redis-cli ping
# Debe responder: PONG

# Iniciar Redis
redis-server

# Linux:
sudo systemctl start redis

# macOS:
brew services start redis
```

### **Error: 'celery' no reconocido (Windows)**

Los scripts de inicio ya usan `python -m celery` en lugar de `celery` para compatibilidad con Windows.

Si necesitas ejecutar Celery manualmente:

```bash
# âœ“ CORRECTO (Windows):
python -m celery -A celery_app.celery_config worker --loglevel=info --pool=solo

# âœ— INCORRECTO:
celery -A celery_app.celery_config worker --loglevel=info
```

### **Error: Botones no funcionan / Web rota**

Verifica que `.env` use `localhost` en lugar de `redis`:

```ini
# âœ“ CORRECTO (instalaciÃ³n local Windows):
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
REDIS_URL=redis://localhost:6379/0

# âœ— INCORRECTO (solo para Docker):
CELERY_BROKER_URL=redis://redis:6379/0
```

### **Error: lxml no se instala (Python 3.14)**

Si usas Python 3.14 (pre-release), lxml puede fallar. Soluciones:

```bash
# OpciÃ³n 1: Actualizar a lxml con binarios
pip install --upgrade lxml

# OpciÃ³n 2: Usar Python 3.10 o 3.11 (recomendado)
```

El `requirements.txt` incluye `lxml>=5.0.0` que tiene binarios para Python 3.14.

### **Chrome no se encuentra**

```bash
# Instalar Chrome/Chromium
# Ubuntu:
sudo apt-get install google-chrome-stable

# macOS:
brew install --cask google-chrome

# Windows:
# Descargar de: https://www.google.com/chrome/

# Instalar ChromeDriver:
# https://chromedriver.chromium.org/
# Debe coincidir con tu versiÃ³n de Chrome
```

### **Error: "No driver available"**

- Todos los drivers estÃ¡n ocupados
- Espera a que se libere uno, o
- Aumenta `DRIVER_POOL_SIZE` en `.env`

```ini
# .env
DRIVER_POOL_SIZE=5  # Aumentar de 3 a 5
```

### **Tareas quedan en "PENDING"**

```bash
# Verificar que Celery worker estÃ¡ corriendo
python -m celery -A celery_app.celery_config inspect active

# Reiniciar worker (Windows):
# Cerrar ventana de Celery y ejecutar de nuevo:
python -m celery -A celery_app.celery_config worker --loglevel=info --pool=solo

# Docker:
docker-compose restart celery_worker
```

### **GPU warnings al iniciar Chrome**

Mensajes como estos son normales y no afectan el funcionamiento:

```
[WARNING]: GPU is not supported
DevToolsActivePort file doesn't exist
```

El scraper funciona correctamente con estos warnings.

---

## ğŸ“ˆ Escalabilidad

### **Escalar Workers**

```yaml
# docker-compose.yml
celery_worker_1:
  # ... config ...
  command: celery -A celery_app.celery_config worker --concurrency=3

celery_worker_2:
  # ... config ...
  command: celery -A celery_app.celery_config worker --concurrency=3

celery_worker_3:
  # ... config ...
  command: celery -A celery_app.celery_config worker --concurrency=3
```

Ahora puedes scrapear **9 perfiles simultÃ¡neamente** (3 workers Ã— 3 concurrency)

### **Escalar en la Nube**

- Deploy workers en mÃºltiples servidores
- Usar Redis en la nube (AWS ElastiCache, Redis Cloud)
- PostgreSQL en vez de SQLite

---

## ğŸ”’ Seguridad

**TODO (para producciÃ³n):**

- [ ] Agregar autenticaciÃ³n (Flask-Login)
- [ ] HTTPS con certificados SSL
- [ ] Rate limiting (Flask-Limiter)
- [ ] ValidaciÃ³n de inputs (Pydantic)
- [ ] CSRF protection
- [ ] Secrets en vault (no en .env)

---

## ğŸ“ Logs

Los logs se guardan en:

```
logs/
â”œâ”€â”€ app.log          # Flask app logs
â””â”€â”€ celery.log       # Celery worker logs
```

Ver logs en tiempo real:

```bash
# Docker
docker-compose logs -f web
docker-compose logs -f celery_worker

# Local
tail -f logs/app.log
```

---

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agrega nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

---

## ğŸ“„ Licencia

MIT License - Ver `LICENSE` para mÃ¡s detalles

---

## ğŸ‰ CrÃ©ditos

Desarrollado con â¤ï¸ por [Tu Nombre]

**Stack TecnolÃ³gico:**
- Flask (Web Framework)
- Celery (Async Tasks)
- Redis (Message Broker)
- Selenium (Web Scraping)
- BeautifulSoup (HTML Parsing)
- Docker (Containerization)

---

## ğŸ“ Soporte

Â¿Problemas? Abre un [Issue](https://github.com/tu-usuario/twitter-scraper/issues)

---

**Â¡Feliz Scraping! ğŸš€**
